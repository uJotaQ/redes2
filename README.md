PBL_REDES - Jogo R√≠tmico Distribu√≠do
Autor: Leonardo Oliveira Almeida da Cruz (@oLeozito) Disciplina: TEC502 - Redes e Conectividade (PBL)

Este projeto implementa um servidor de jogo multiplayer distribu√≠do, tolerante a falhas e escal√°vel, como parte da avalia√ß√£o da disciplina. O sistema foi projetado para migrar uma arquitetura de servidor de jogo centralizado para um cluster de n√≥s distribu√≠dos que gerenciam o estado do jogo de forma consistente.

üöÄ Tecnologias Utilizadas
Go (Golang): Linguagem principal para o desenvolvimento do servidor e cliente.

Protocolo Raft: Utilizado para consenso e replica√ß√£o de estado (via biblioteca hashicorp/raft).

TCP Sockets: Protocolo de comunica√ß√£o prim√°rio entre o cliente (cliente.go) e o n√≥ do servidor (servidor.go) ao qual ele se conecta.

API REST (HTTP): Protocolo de comunica√ß√£o servidor-servidor, usado para encaminhamento de requisi√ß√µes e broadcast de notifica√ß√µes.

JSON: Formato de serializa√ß√£o para todas as mensagens TCP e REST.

Docker & Docker Compose: Utilizado para orquestrar e testar o ambiente distribu√≠do com m√∫ltiplos n√≥s de servidor.

üèõÔ∏è Vis√£o Geral da Arquitetura (Barema - Item 1)
A arquitetura do sistema √© baseada no padr√£o de M√°quina de Estados Replicada (Replicated State Machine), utilizando o algoritmo de consenso Raft para garantir a consist√™ncia.

Diferente de uma arquitetura centralizada, onde um √∫nico servidor det√©m todo o estado (jogadores, salas, invent√°rios), esta solu√ß√£o utiliza um cluster de n√≥s servidor.go.

N√≥s do Cluster: Qualquer inst√¢ncia do servidor.go pode ser executada como um n√≥. Os n√≥s se comunicam para eleger um L√≠der.

L√≠der vs. Seguidor:

O L√≠der √© o √∫nico n√≥ autorizado a aplicar mudan√ßas de estado (ex: cadastrar um jogador, comprar um pacote, jogar uma nota).

Os Seguidores (Followers) atuam como gateways leves. Eles recebem conex√µes TCP dos clientes, mas, em vez de processar as l√≥gicas de neg√≥cio, eles encaminham a requisi√ß√£o para o L√≠der via API REST.

Fluxo de Dados (FSM):

Um cliente (ex: cliente1) envia um comando (ex: COMPRAR_PACOTE) via TCP para o n√≥ ao qual est√° conectado (ex: Servidor 2 - Seguidor).

O Servidor 2 encaminha essa requisi√ß√£o para o Servidor 1 - L√≠der via REST (/request-purchase).

O L√≠der recebe a requisi√ß√£o, a submete ao log do Raft (raftNode.Apply()).

O Raft garante que este comando seja replicado e aplicado na M√°quina de Estados Finitos (FSM) de todos os n√≥s do cluster (incluindo o Servidor 2).

A FSM (fun√ß√£o FSM.Apply) executa a l√≥gica de forma determin√≠stica, garantindo que o estoque de pacotes e o invent√°rio do jogador sejam atualizados de forma id√™ntica em todos os n√≥s.

Essa arquitetura permite escalabilidade horizontal: podemos adicionar mais n√≥s seguidores para lidar com mais conex√µes de clientes, sem sobrecarregar o n√≥ L√≠der, que se concentra apenas em orquestrar o estado.

üìã Respostas Detalhadas ao Barema
Aqui detalhamos como cada requisito de avalia√ß√£o foi implementado.

2. Comunica√ß√£o Servidor-Servidor
A comunica√ß√£o entre os n√≥s do servidor √© realizada atrav√©s de duas vias principais:

Raft (TCP): O hashicorp/raft gerencia sua pr√≥pria comunica√ß√£o interna (bin√°ria) para elei√ß√£o de l√≠der e replica√ß√£o de log.

API REST (HTTP): Uma API REST (startHttpApi) √© exposta por cada n√≥ para permitir a colabora√ß√£o e o encaminhamento de requisi√ß√µes.

Endpoints REST Principais:

/join: (POST) Endpoint de bootstrap. Um novo n√≥ o utiliza para solicitar ao L√≠der sua adi√ß√£o ao cluster Raft.

/request-register, /request-login, /request-logout, /request-reconnect: (POST) Usados por seguidores para encaminhar comandos de autentica√ß√£o e sess√£o para o L√≠der.

/request-purchase, /request-trade: (POST) Usados por seguidores para encaminhar l√≥gicas de neg√≥cio cr√≠ticas (compra e troca) para o L√≠der, garantindo que sejam processadas pela FSM.

/request-create-room, /request-find-room, /request-play-note: (POST) Usados por seguidores para encaminhar toda a l√≥gica de pareamento e jogabilidade para o L√≠der.

/notify-player: (POST) Endpoint crucial usado pelo L√≠der para transmitir (broadcast) eventos a todos os seguidores. Por exemplo, quando uma troca √© conclu√≠da, o L√≠der envia a notifica√ß√£o para este endpoint em todos os outros n√≥s, e cada n√≥ √© respons√°vel por verificar se o jogador-alvo est√° conectado a ele e entregar a mensagem via TCP.

3. Comunica√ß√£o Cliente-Servidor
O barema sugere um modelo publisher-subscriber (como MQTT). A solu√ß√£o implementada atinge o mesmo objetivo (notifica√ß√µes push do servidor para o cliente) de forma mais integrada, usando uma combina√ß√£o de TCP e REST.

Canal Prim√°rio (TCP): A comunica√ß√£o √© bidirecional sobre uma √∫nica conex√£o TCP (handleConnection). O cliente envia comandos (ex: PLAY_NOTE) e o servidor responde diretamente (ex: SCREEN_MSG de erro).

Modelo "Pub/Sub" Simulado (Push): Para eventos que n√£o s√£o uma resposta direta (como o in√≠cio do turno do oponente ou uma proposta de troca), o sistema simula um Pub/Sub:

O L√≠der (Publisher) decide que um evento deve ocorrer (ex: TURN_UPDATE).

Ele publica este evento para todos os n√≥s seguidores via REST (usando o endpoint /notify-player).

Cada n√≥ (Seguidor) age como um broker local. Ele verifica se o jogador-alvo (Subscriber) est√° conectado a ele.

Se estiver, o n√≥ envia (push) a mensagem (TURN_UPDATE) pela conex√£o TCP daquele cliente espec√≠fico.

Esta abordagem evita a necessidade de um broker MQTT externo, integrando o broadcast de mensagens √† pr√≥pria l√≥gica de estado do Raft, e usando o TCP (que j√° √© usado para comandos) como o canal de entrega.

4. Gerenciamento Distribu√≠do de Estoque
Este √© um dos pontos mais cr√≠ticos do sistema e √© resolvido integralmente pelo Raft.

Problema: Como garantir que dois jogadores em servidores diferentes n√£o comprem o mesmo pacote √∫nico (ex: packet_ID_001) ao mesmo tempo?

Solu√ß√£o (Consenso do L√≠der):

O "estoque global" (packetStock) √© um mapa replicado na FSM de todos os n√≥s.

Quando um Jogador 1 (no Servidor A) e um Jogador 2 (no Servidor B) tentam comprar um pacote simultaneamente, ambas as requisi√ß√µes s√£o encaminhadas para o L√≠der.

O L√≠der lineariza (enfileira) essas requisi√ß√µes. Ele aplica uma de cada vez ao log do Raft.

A FSM (no case "COMPRAR_PACOTE") √© executada:

Ela bloqueia o estoque (stockMu.Lock()).

Pega o primeiro pacote dispon√≠vel (ex: availablePacketIDs[0]).

Marca o pacote como Opened = true.

Adiciona o pacote ao invent√°rio do jogador.

Desbloqueia o estoque.

Quando a FSM processar a requisi√ß√£o do Jogador 2, o pacote que o Jogador 1 pegou n√£o estar√° mais dispon√≠vel. A FSM pegar√° o pr√≥ximo pacote da lista.

Como o Raft garante que o log √© aplicado na mesma ordem em todos os lugares, √© criptograficamente imposs√≠vel que dois jogadores recebam o mesmo item, garantindo a justi√ßa e consist√™ncia do estoque.

5. Consist√™ncia e Justi√ßa do Estado do Jogo
Assim como o estoque, todo o estado do jogo (saldo, invent√°rios, progresso da partida) √© gerenciado pela FSM e protegido pelo Raft.

Exemplo: Troca de Cartas (PROPOSE_TRADE) A l√≥gica de troca √© totalmente at√¥mica dentro da FSM:

Um jogador (P1) prop√µe trocar a Carta A por uma Carta B de outro jogador (P2).

A requisi√ß√£o √© enviada ao L√≠der e aplicada na FSM.

A FSM verifica o mapa activeTrades para ver se (P2) j√° ofereceu a Carta B pela Carta A (uma reverseTradeKey).

Se a oferta reversa existe: A FSM executa a troca atomica e instantaneamente. Ela remove a Carta A de P1, remove a Carta B de P2, e as adiciona aos invent√°rios opostos. Isso √© uma transa√ß√£o √∫nica.

Se n√£o existe: A FSM apenas registra a oferta de P1 no mapa activeTrades.

Exemplo: Partida (PLAY_NOTE_RAFT) O estado da partida (GameState) tamb√©m √© gerenciado pela FSM. Quando um jogador joga uma nota:

A FSM verifica se √© o turno do jogador (sala.Game.CurrentTurn == playerNum).

Ela adiciona a nota √† sequ√™ncia (sala.Game.PlayedNotes).

Verifica se um ataque foi completado (checkAttackCompletionFSM).

Atualiza o turno (sala.Game.CurrentTurn = 2).

Como o L√≠der Raft √© a √∫nica fonte da verdade para a ordem desses eventos, o estado do jogo √© sempre fortemente consistente.

6. Pareamento em Ambiente Distribu√≠do
O pareamento funciona independentemente de onde os jogadores est√£o conectados, pois o "lobby" (salasEmEspera e salas) √© um estado replicado pela FSM.

Cria√ß√£o (P√∫blica): Jogador 1 (no Servidor A) envia FIND_ROOM (sem sala de espera). A requisi√ß√£o vai ao L√≠der, que aplica CREATE_ROOM_RAFT. A FSM adiciona uma nova sala ao mapa salasEmEspera.

Entrada (P√∫blica): Jogador 2 (no Servidor B) envia FIND_ROOM. A requisi√ß√£o vai ao L√≠der. O L√≠der v√™ que salasEmEspera n√£o est√° vazio. Ele aplica JOIN_ROOM_RAFT.

L√≥gica do JOIN_ROOM_RAFT (na FSM):

A FSM remove a sala de salasEmEspera.

Define o Status da sala como "Em_Jogo".

Define P2Login como o Jogador 2.

Inicializa o sala.Game.

Notifica√ß√£o (Handler do L√≠der):

Ap√≥s a FSM ser aplicada, o handler HTTP do L√≠der (ex: handleJoinRoomResultREST) √© notificado do sucesso.

Ele envia a mensagem PAREADO de volta ao Jogador 2 (pela resposta HTTP ao Servidor B).

Ele difunde (broadcast) a mensagem PAREADO para o Jogador 1 (via /notify-player para o Servidor A).

A FSM garante que a sala seja preenchida apenas uma vez (pareamento √∫nico), e a camada HTTP/REST do L√≠der garante que ambos os jogadores (em servidores diferentes) sejam notificados.

7. Toler√¢ncia a Falhas e Resili√™ncia
O sistema √© resiliente a falhas de N√≥s (Servidores) e Clientes.

Resili√™ncia a Falha de N√≥ (Servidor):

Falha de Seguidor: Se um n√≥ seguidor falhar, o cluster continua operando normalmente. Os clientes conectados a ele perder√£o a conex√£o, mas poder√£o se reconectar a outro seguidor (veja abaixo).

Falha de L√≠der: Se o n√≥ L√≠der falhar, os seguidores restantes (desde que formem um qu√≥rum, ex: 2 de 3) ir√£o detectar a falha, iniciar uma nova elei√ß√£o e promover um novo L√≠der em milissegundos. O servi√ßo continua sem interrup√ß√£o.

Resili√™ncia a Falha de Conex√£o (Cliente): O cliente.go foi projetado para sobreviver a falhas de conex√£o:

connectToServer: O cliente possui uma lista de todos os endere√ßos de servidor. Ao iniciar, ele embaralha a lista e tenta se conectar a qualquer um deles.

ReconnectLoop: Se a conex√£o TCP cair (ex: o n√≥ seguidor falhou), o main() do cliente entra em um loop, tentando se reconectar a qualquer outro n√≥ da lista.

RECONNECT_SESSION: Ap√≥s reconectar (a um n√≥ diferente), o cliente envia o comando RECONNECT_SESSION com seu login. O servidor (L√≠der) recebe isso via FSM (RECONNECT_SESSION_RAFT), valida que o jogador estava Online = true, e o handler no novo n√≥ seguidor associa a nova conex√£o TCP ao estado do jogador (player.Conn = conn).

Isso permite que um jogador perca a conex√£o, se reconecte a um servidor totalmente diferente e continue sua sess√£o (e at√© mesmo seu jogo, se a l√≥gica fosse expandida) sem precisar fazer login novamente.

‚öôÔ∏è Como Executar (Docker - Item 9)
O docker-compose.yml e o Makefile orquestram o ambiente distribu√≠do.

1. Construir e Iniciar o Cluster (3 N√≥s):

O docker-compose.yml est√° configurado para iniciar 3 n√≥s de servidor e un√≠-los em um cluster.

Bash

# Constr√≥i as imagens e inicia os cont√™ineres
make all

# Ou, para iniciar em background:
docker-compose up --build -d
2. Visualizar os Logs:

Voc√™ ver√° os 3 servidores iniciando, realizando uma elei√ß√£o e um deles se declarando L√çDER.

Bash

docker-compose logs -f
3. Escalar o Cluster:

Para adicionar mais seguidores (escalabilidade):

Bash

docker-compose up --build -d --scale servidor=5
4. Rodar o Cliente:

O cliente n√£o est√° em Docker. Voc√™ pode rod√°-lo localmente no seu terminal:

Bash

# Estando na raiz do projeto (PBL_REDES/)
go run ./cmd/cliente/cliente.go
O cliente tentar√° se conectar √†s portas expostas pelos cont√™ineres Docker (ex: 8081, 8082, 8083).

5. Encerrar o Ambiente:

Bash

make down
üß™ Como Testar (Testes de Software - Item 8)
Foram desenvolvidos testes de integra√ß√£o automatizados (cmd/servidor/servidor_test.go) que validam o sistema em um cen√°rio realista.

O script de teste:

Compila e inicia um bin√°rio real do servidor.go como um n√≥ √∫nico (que se torna L√≠der).

Espera ativamente que o servidor se torne L√çDER (consultando o endpoint /health).

Inicia "clientes de teste" (TestClient) que se conectam via TCP.

Executa os seguintes cen√°rios:

TestCadastroELogin: Testa o cadastro de um novo usu√°rio, o login bem-sucedido, a falha de cadastro duplicado e a falha de senha incorreta.

TestCompraPacote: Testa a compra de pacotes, a verifica√ß√£o de saldo (deve diminuir) e a falha de compra por saldo insuficiente (esgotamento).

TestPartidaSimples: Simula uma partida completa com dois jogadores, incluindo compra de instrumentos, sele√ß√£o, cria√ß√£o de sala, pareamento e uma rodada de jogo.

Para rodar os testes:

Bash

# 1. Navegue at√© o diret√≥rio do servidor
cd cmd/servidor

# 2. Execute o Go Test
# (O -v mostra os testes passando, e o -timeout √© por seguran√ßa)
go test -v -timeout 60s
Resultado esperado:

=== RUN   TestCadastroELogin
=== RUN   TestCadastroELogin/Cadastro
=== RUN   TestCadastroELogin/LoginCorreto
=== RUN   TestCadastroELogin/CadastroRepetido
=== RUN   TestCadastroELogin/LoginSenhaErrada
--- PASS: TestCadastroELogin (0.01s)
    --- PASS: TestCadastroELogin/Cadastro (0.00s)
    --- PASS: TestCadastroELogin/LoginCorreto (0.00s)
    --- PASS: TestCadastroELogin/CadastroRepetido (0.00s)
    --- PASS: TestCadastroELogin/LoginSenhaErrada (0.00s)
=== RUN   TestCompraPacote
=== RUN   TestCompraPacote/CompraComSaldo
=== RUN   TestCompraPacote/ChecarSaldoPosCompra
=== RUN   TestCompraPacote/EsgotarSaldo
--- PASS: TestCompraPacote (0.00s)
    --- PASS: TestCompraPacote/CompraComSaldo (0.00s)
    --- PASS: TestCompraPacote/ChecarSaldoPosCompra (0.00s)
    --- PASS: TestCompraPacote/EsgotarSaldo (0.00s)
=== RUN   TestPartidaSimples
=== RUN   TestPartidaSimples/CriarSala
=== RUN   TestPartidaSimples/EntrarSalaEIniciar
    servidor_test.go:497: Partida de um turno conclu√≠da com sucesso.
--- PASS: TestPartidaSimples (0.00s)
    --- PASS: TestPartidaSimples/CriarSala (0.00s)
    --- PASS: TestPartidaSimples/EntrarSalaEIniciar (0.00s)
PASS
ok      pbl_redes/server        1.234s
